# ğŸ¥ Health Insurance Rate Data Engineering Pipeline

This project provides a modular, production-grade data engineering pipeline to ingest, transform, and enrich U.S. health insurance rate data using Spark and Python. It follows the Bronze â†’ Silver â†’ Gold architecture and includes optional orchestration using [Papermill](https://papermill.readthedocs.io/en/latest/).


## ğŸ“ Folder Structure

```
health-insurance-rate-data-etl/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE                  # MIT license (see below)
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ gold/
â”‚   â””â”€â”€ meta_data_files/  
â”‚ 
â”œâ”€â”€ configurations/
â”‚   â””â”€â”€ config.json          # Paths and filenames config
â”‚   â”œâ”€â”€ api_config.yaml      # API source configuration
â”‚
â”œâ”€â”€ notebooks/               # Jupyter notebooks for each pipeline stage
â”‚   â”œâ”€â”€ ingest_bronze.ipynb
â”‚   â”œâ”€â”€ transform_silver.ipynb
â”‚   â”œâ”€â”€ enrich_gold.ipynb
â”‚   â””â”€â”€ output/              # Auto-generated by orchestrate.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ dataframe_utils.py     # Spark helpers
â”‚   â”‚   â”œâ”€â”€ request_utils.py       # API fetchers
â”‚   â”‚   â”œâ”€â”€ data_quality_utils.py  # Quality rules
â”‚   â”‚   â””â”€â”€ path_utils.py          # Project path helpers
â”‚
â”œâ”€â”€ orchestrate.py           # Papermill-based orchestration

```

## ğŸš€ Features

- ğŸ“¥ API Ingestion via YAML-configured endpoint
- ğŸ§¹ Silver-level cleaning and null handling
- ğŸ§¾ Gold-level enrichment with derived columns
- ğŸ““ Notebook-driven ETL for transparency
- ğŸ“Š Data Quality Checks included
- âš™ï¸ Papermill-based orchestration
- ğŸ” Configurable paths and filenames via JSON


## ğŸ”§ Setup

### ğŸ”¹ Create Environment

```bash
conda create --name <<your-env-name>> python=3.10
conda activate <<your-env-name>>
pip install -r requirements.txt
```

> Optional: For running notebooks interactively
```bash
conda install ipykernel
python -m ipykernel install --user --name=<<your-env-name>> --display-name "<<your-env-name>>"
```

## â˜• Java Requirements

Ensure Java is available for Spark (Java 17) and paths are set

## ğŸ§ª Running the Pipeline

### â–¶ï¸ Option 1: Via Notebooks

Run each notebook manually:

- `ingest_bronze.ipynb`
- `transform_silver.ipynb`
- `enrich_gold.ipynb`

### â–¶ï¸ Option 2: Papermill Orchestration

```bash
python orchestrate.py
```

This will inject paths and execute each notebook in order.


## ğŸ“Š Data Quality Checks

Implemented in `src/utils/data_quality_utils.py`:

- Check for nulls in critical fields
- Ensure valid age ranges and 2-letter state codes
- Raise descriptive exceptions if checks fail


## ğŸ“œ License

This project is licensed under the MIT License.  
See the [MIT License](LICENSE) file for full terms.

## ğŸ¤ Contributing

Feel free to fork, suggest improvements, or open issues.  
This is designed as an open learning resource and modular data engineering foundation.
