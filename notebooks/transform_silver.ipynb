{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f2566c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))  # adds the parent folder to sys.path\n",
    "from src.utils import dataframe_utils\n",
    "from src.utils.path_utils import find_project_root,Path\n",
    "from src.utils.dataquality import data_quality_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2943b60",
   "metadata": {},
   "source": [
    "### Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c135d",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "base_dir='../'\n",
    "config_path=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf27d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if config_path is not None:\n",
    "    BASE_DIR = Path(base_dir)\n",
    "    config_path\n",
    "else:\n",
    "    # fallback: find project root and load config.json\n",
    "    BASE_DIR = find_project_root()\n",
    "    config_path = BASE_DIR / 'configurations' / 'config.json'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded config from {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16518aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_file_name='Rate_PUF.csv'\n",
    "raw_csv_path = os.path.join(BASE_DIR,config[\"download_dir\"], downloaded_file_name)\n",
    "silver_path = os.path.join(BASE_DIR,config[\"silver_parquet_path\"], downloaded_file_name.split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d553e",
   "metadata": {},
   "source": [
    "### Cleaning/Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323822a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read using PySpark\n",
    "read_df=dataframe_utils.read_data_spark(file_path=raw_csv_path,\n",
    "                                          file_format=\"csv\",\n",
    "                                          header=True,\n",
    "                                          inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2beb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to string type\n",
    "rates_df = read_df.select([fun.col(c).cast(\"string\") for c in read_df.columns])\n",
    "\n",
    "# Fill all null values with empty strings\n",
    "silver_rates_df = rates_df.fillna(\"\")\n",
    "\n",
    "# Get today's date in YYYYMMDD format\n",
    "today_str = datetime.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Add column with the same date for all rows\n",
    "silver_rates_df = rates_df.withColumn(\"ImportDate\", fun.lit(today_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as Silver dataset\n",
    "dataframe_utils.write_data_spark(file_path=silver_path,\n",
    "                                 file_format='parquet',\n",
    "                                 df=silver_rates_df.coalesce(4),  # Adjust 4 to a lower number if needed or any heap memory issues\n",
    "                                 mode='overwrite',\n",
    "                                 partition_by=['ImportDate','StateCode','Age'],\n",
    "                                 header=True)\n",
    "\n",
    "print(f\"Saved silver parquet at {silver_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a6241",
   "metadata": {},
   "source": [
    "### Data Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_quality_checks(silver_rates_df):\n",
    "    print(\"All Data Quality Checks Passed âœ…\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hinsurance-etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
